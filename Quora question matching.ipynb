{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Quora question matching.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1R_FJBgXJo5QG19Oen4Bg6dq11ofiexPM","authorship_tag":"ABX9TyPQMIuPfG3zZP9GhWt2RykX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"oB-MAr4oNLnc","colab_type":"code","colab":{}},"source":["import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-KTNwabNmtm","colab_type":"code","colab":{}},"source":["os.chdir('/content/drive/My Drive/Colab Notebooks/Cloud 493/sentiment')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4hdHgxJGNOos","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"cf8e4320-9664-4764-85c4-6694aa503ec5","executionInfo":{"status":"ok","timestamp":1589258946310,"user_tz":420,"elapsed":34702,"user":{"displayName":"Aravind Venugopal","photoUrl":"","userId":"09108547859103269258"}}},"source":["#!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2020-05-12 04:48:30--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.146.53\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.146.53|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1647046227 (1.5G) [application/x-gzip]\n","Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n","\n","GoogleNews-vectors- 100%[===================>]   1.53G  48.1MB/s    in 33s     \n","\n","2020-05-12 04:49:04 (47.3 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Uu6Tslx9Wkke","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b9bdea91-8b64-4495-ff3f-e93156190c1c","executionInfo":{"status":"ok","timestamp":1589280970496,"user_tz":420,"elapsed":3584,"user":{"displayName":"Aravind Venugopal","photoUrl":"","userId":"09108547859103269258"}}},"source":["'''\n","Single model may achieve LB scores at around 0.29+ ~ 0.30+\n","Average ensembles can easily get 0.28+ or less\n","Don't need to be an expert of feature engineering\n","All you need is a GPU!!!!!!!\n","\n","The code is tested on Keras 2.0.0 using Tensorflow backend, and Python 2.7\n","\n","According to experiments by kagglers, Theano backend with GPU may give bad LB scores while\n","        the val_loss seems to be fine, so try Tensorflow backend first please\n","'''\n","\n","########################################\n","## import packages\n","########################################\n","import os\n","import re\n","import csv\n","import codecs\n","import numpy as np\n","import pandas as pd\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import SnowballStemmer\n","from string import punctuation\n","\n","from gensim.models import KeyedVectors\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers.merge import concatenate\n","from keras.models import Model\n","from keras.layers.normalization import BatchNormalization\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"haxRzKdQZeTv","colab_type":"code","colab":{}},"source":["def read_file(path, thresh):\n","  df = pd.read_csv(path, encoding='utf-8')\n","  thresh = int(round(len(df)/thresh,0))\n","  df = df.head(thresh)\n","  texts_1 = df['question1'].apply(text_to_wordlist).values.tolist()\n","  texts_2 = df['question2'].apply(text_to_wordlist).values.tolist()\n","  if 'test' in path:\n","    labels = df.iloc[:,0].astype(int).values.tolist()\n","  else:\n","    labels = df['is_duplicate'].astype(int).values.tolist()\n","  return texts_1, texts_2, labels"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUMme70YLmDv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":479},"outputId":"3c8593d9-e8f3-406b-e518-3e4690b85234","executionInfo":{"status":"ok","timestamp":1589281447917,"user_tz":420,"elapsed":271896,"user":{"displayName":"Aravind Venugopal","photoUrl":"","userId":"09108547859103269258"}}},"source":["# import sys\n","# reload(sys)\n","# sys.setdefaultencoding('utf-8')\n","\n","########################################\n","## set directories and parameters\n","########################################\n","BASE_DIR = '/content/drive/My Drive/Colab Notebooks/Cloud 493/'\n","EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin.gz'\n","TRAIN_DATA_FILE = BASE_DIR + 'train.csv'\n","TEST_DATA_FILE = BASE_DIR + 'test.csv'\n","MAX_SEQUENCE_LENGTH = 30\n","MAX_NB_WORDS = 200000\n","EMBEDDING_DIM = 300\n","VALIDATION_SPLIT = 0.1\n","\n","num_lstm = np.random.randint(175, 275)\n","num_dense = np.random.randint(100, 150)\n","rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n","rate_drop_dense = 0.15 + np.random.rand() * 0.25\n","\n","act = 'relu'\n","re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n","\n","STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n","        rate_drop_dense)\n","\n","########################################\n","## index word vectors\n","########################################\n","print('Indexing word vectors')\n","\n","word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n","        binary=True)\n","print('Found %s word vectors of word2vec' % len(word2vec.vocab))\n","\n","########################################\n","## process texts in datasets\n","########################################\n","print('Processing text dataset')\n","\n","# The function \"text_to_wordlist\" is from\n","# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n","def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n","    # Clean the text, with the option to remove stopwords and to stem words.\n","    \n","    # Convert words to lower case and split them\n","    text = text.lower().split()\n","\n","    # Optionally, remove stop words\n","    if remove_stopwords:\n","        stops = set(stopwords.words(\"english\"))\n","        text = [w for w in text if not w in stops]\n","    \n","    text = \" \".join(text)\n","\n","    # Clean the text\n","    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n","    text = re.sub(r\"what's\", \"what is \", text)\n","    text = re.sub(r\"\\'s\", \" \", text)\n","    text = re.sub(r\"\\'ve\", \" have \", text)\n","    text = re.sub(r\"can't\", \"cannot \", text)\n","    text = re.sub(r\"n't\", \" not \", text)\n","    text = re.sub(r\"i'm\", \"i am \", text)\n","    text = re.sub(r\"\\'re\", \" are \", text)\n","    text = re.sub(r\"\\'d\", \" would \", text)\n","    text = re.sub(r\"\\'ll\", \" will \", text)\n","    text = re.sub(r\",\", \" \", text)\n","    text = re.sub(r\"\\.\", \" \", text)\n","    text = re.sub(r\"!\", \" ! \", text)\n","    text = re.sub(r\"\\/\", \" \", text)\n","    text = re.sub(r\"\\^\", \" ^ \", text)\n","    text = re.sub(r\"\\+\", \" + \", text)\n","    text = re.sub(r\"\\-\", \" - \", text)\n","    text = re.sub(r\"\\=\", \" = \", text)\n","    text = re.sub(r\"'\", \" \", text)\n","    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n","    text = re.sub(r\":\", \" : \", text)\n","    text = re.sub(r\" e g \", \" eg \", text)\n","    text = re.sub(r\" b g \", \" bg \", text)\n","    text = re.sub(r\" u s \", \" american \", text)\n","    text = re.sub(r\"\\0s\", \"0\", text)\n","    text = re.sub(r\" 9 11 \", \"911\", text)\n","    text = re.sub(r\"e - mail\", \"email\", text)\n","    text = re.sub(r\"j k\", \"jk\", text)\n","    text = re.sub(r\"\\s{2,}\", \" \", text)\n","    \n","    # Optionally, shorten words to their stems\n","    if stem_words:\n","        text = text.split()\n","        stemmer = SnowballStemmer('english')\n","        stemmed_words = [stemmer.stem(word) for word in text]\n","        text = \" \".join(stemmed_words)\n","    \n","    # Return a list of words\n","    return(text)\n","\n","# texts_1 = [] \n","# texts_2 = []\n","# labels = []\n","# with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n","#     reader = csv.reader(f, delimiter=',')\n","#     header = next(reader)\n","#     for values in reader:\n","#         texts_1.append(text_to_wordlist(values[3]))\n","#         texts_2.append(text_to_wordlist(values[4]))\n","#         labels.append(int(values[5]))\n","\n","texts_1, texts_2, labels = read_file(TRAIN_DATA_FILE,thresh = 10)\n","print('Found %s texts in train.csv' % len(texts_1))\n","\n","# test_texts_1 = []\n","# test_texts_2 = []\n","# test_ids = []\n","# with codecs.open(TEST_DATA_FILE, encoding='utf-8') as f:\n","#     reader = csv.reader(f, delimiter=',')\n","#     header = next(reader)\n","#     for values in reader:\n","#         test_texts_1.append(text_to_wordlist(values[1]))\n","#         test_texts_2.append(text_to_wordlist(values[2]))\n","#         test_ids.append(values[0])\n","test_texts_1, test_texts_2, test_ids = read_file(TEST_DATA_FILE,thresh = 10)\n","print('Found %s texts in test.csv' % len(test_texts_1))\n","\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n","\n","sequences_1 = tokenizer.texts_to_sequences(texts_1)\n","sequences_2 = tokenizer.texts_to_sequences(texts_2)\n","test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n","test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens' % len(word_index))\n","\n","data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n","data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n","labels = np.array(labels)\n","print('Shape of data tensor:', data_1.shape)\n","print('Shape of label tensor:', labels.shape)\n","\n","test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n","test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n","test_ids = np.array(test_ids)\n","\n","########################################\n","## prepare embeddings\n","########################################\n","print('Preparing embedding matrix')\n","\n","nb_words = min(MAX_NB_WORDS, len(word_index))+1\n","\n","embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    if word in word2vec.vocab:\n","        embedding_matrix[i] = word2vec.word_vec(word)\n","print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n","\n","########################################\n","## sample train/validation data\n","########################################\n","#np.random.seed(1234)\n","perm = np.random.permutation(len(data_1))\n","idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n","idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n","\n","data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n","data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n","labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n","\n","data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n","data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n","labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n","\n","weight_val = np.ones(len(labels_val))\n","if re_weight:\n","    weight_val *= 0.472001959\n","    weight_val[labels_val==0] = 1.309028344\n","\n","########################################\n","## define the model structure\n","########################################\n","embedding_layer = Embedding(nb_words,\n","        EMBEDDING_DIM,\n","        weights=[embedding_matrix],\n","        input_length=MAX_SEQUENCE_LENGTH,\n","        trainable=False)\n","lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n","\n","sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n","embedded_sequences_1 = embedding_layer(sequence_1_input)\n","x1 = lstm_layer(embedded_sequences_1)\n","\n","sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n","embedded_sequences_2 = embedding_layer(sequence_2_input)\n","y1 = lstm_layer(embedded_sequences_2)\n","\n","merged = concatenate([x1, y1])\n","merged = Dropout(rate_drop_dense)(merged)\n","merged = BatchNormalization()(merged)\n","\n","merged = Dense(num_dense, activation=act)(merged)\n","merged = Dropout(rate_drop_dense)(merged)\n","merged = BatchNormalization()(merged)\n","\n","preds = Dense(1, activation='sigmoid')(merged)\n","\n","########################################\n","## add class weight\n","########################################\n","if re_weight:\n","    class_weight = {0: 1.309028344, 1: 0.472001959}\n","else:\n","    class_weight = None\n","\n","########################################\n","## train the model\n","########################################\n","model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n","        outputs=preds)\n","model.compile(loss='binary_crossentropy',\n","        optimizer='nadam',\n","        metrics=['acc'])\n","#model.summary()\n","print(STAMP)\n","\n","early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n","bst_model_path = STAMP + '.h5'\n","model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=False)\n","\n","hist = model.fit([data_1_train, data_2_train], labels_train, \\\n","        validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n","        epochs=200, batch_size=2048, shuffle=True, \\\n","        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n","\n","model.load_weights(bst_model_path)\n","bst_val_score = min(hist.history['val_loss'])\n","model.save('/content/drive/My Drive/Colab Notebooks/Cloud 493/quora-similarity-model')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Indexing word vectors\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"stream","text":["Found 3000000 word vectors of word2vec\n","Processing text dataset\n","Found 40429 texts in train.csv\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n","  if self.run_code(code, result):\n"],"name":"stderr"},{"output_type":"stream","text":["Found 356348 texts in test.csv\n","Found 78459 unique tokens\n","Shape of data tensor: (40429, 30)\n","Shape of label tensor: (40429,)\n","Preparing embedding matrix\n","Null word embeddings: 33028\n","lstm_236_110_0.19_0.36\n","Train on 72772 samples, validate on 8086 samples\n","Epoch 1/200\n","72772/72772 [==============================] - 17s 229us/step - loss: 0.6420 - acc: 0.6303 - val_loss: 0.4842 - val_acc: 0.6254\n","Epoch 2/200\n","72772/72772 [==============================] - 14s 192us/step - loss: 0.4567 - acc: 0.6863 - val_loss: 0.4496 - val_acc: 0.6321\n","Epoch 3/200\n","72772/72772 [==============================] - 14s 192us/step - loss: 0.4067 - acc: 0.6788 - val_loss: 0.4557 - val_acc: 0.6278\n","Epoch 4/200\n","72772/72772 [==============================] - 14s 192us/step - loss: 0.3906 - acc: 0.6847 - val_loss: 0.4582 - val_acc: 0.6294\n","Epoch 5/200\n","72772/72772 [==============================] - 14s 192us/step - loss: 0.3807 - acc: 0.6909 - val_loss: 0.4554 - val_acc: 0.6284\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XXLDlbCOWrEM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"0c744f96-739a-4611-ccfb-2696c7ea67f3","executionInfo":{"status":"ok","timestamp":1589264522812,"user_tz":420,"elapsed":44940,"user":{"displayName":"Aravind Venugopal","photoUrl":"","userId":"09108547859103269258"}}},"source":["\n","########################################\n","## make the submission\n","########################################\n","print('Start making the submission before fine-tuning')\n","\n","preds = model.predict([test_data_1, test_data_2], batch_size=8192, verbose=1)\n","preds += model.predict([test_data_2, test_data_1], batch_size=8192, verbose=1)\n","preds /= 2\n","\n","submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n","# submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Start making the submission before fine-tuning\n","356348/356348 [==============================] - 22s 63us/step\n","356348/356348 [==============================] - 22s 62us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"62oCzJnVmOLO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"4680432d-c909-4a2d-a71b-034116794796","executionInfo":{"status":"ok","timestamp":1589265899784,"user_tz":420,"elapsed":376,"user":{"displayName":"Aravind Venugopal","photoUrl":"","userId":"09108547859103269258"}}},"source":["print(test_texts_1[:2],'\\n',test_texts_2[:2])"],"execution_count":20,"outputs":[{"output_type":"stream","text":["['how does the surface pro himself 4 compare with ipad pro ', 'should i have a hair transplant at age 24 how much would it cost '] \n"," ['why did microsoft choose core m3 and not core i3 home surface pro 4 ', 'how much cost does hair transplant require ']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5SecqMXgoUpw","colab_type":"code","colab":{}},"source":["compare = ['how does the surface pro himself 4 compare with ipad pro','why did microsoft choose core m3 and not core i3 home surface pro 4']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0RRQWt20ocwc","colab_type":"code","colab":{}},"source":["compare = [text_to_wordlist(a) for a in compare]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jk9Vrm1qjC57","colab_type":"code","colab":{}},"source":["from keras.preprocessing.text import tokenizer_from_json\n","import json\n","MAX_SEQUENCE_LENGTH = 30\n","MAX_NB_WORDS = 200000\n","with open('/content/drive/My Drive/Colab Notebooks/Cloud 493/tokenizer.json', 'r') as f:\n","    tokenizer_data = json.loads(f.read())\n","tokenizer = tokenizer_from_json(tokenizer_data)\n","test_sequences_1 = tokenizer.texts_to_sequences([compare[0]])\n","test_sequences_2 = tokenizer.texts_to_sequences([compare[1]])\n","test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n","test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eyR-mSRNorQ6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"a385db0e-b28c-44da-acb6-29c90ba4d14a","executionInfo":{"status":"ok","timestamp":1589287209226,"user_tz":420,"elapsed":838,"user":{"displayName":"Aravind Venugopal","photoUrl":"","userId":"09108547859103269258"}}},"source":["res = model.predict([test_data_1.tolist(), test_data_2.tolist()], batch_size=8192, verbose=1)\n","0 if res[0,0] <= 0.5 else 1"],"execution_count":19,"outputs":[{"output_type":"stream","text":["\r1/1 [==============================] - 0s 26ms/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"TIFqoqgMxqnp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a9486a90-db3b-4030-a3ed-f516e577abc4","executionInfo":{"status":"ok","timestamp":1589287193546,"user_tz":420,"elapsed":867,"user":{"displayName":"Aravind Venugopal","photoUrl":"","userId":"09108547859103269258"}}},"source":["type(test_data_1.tolist())"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["list"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"3DtmzTrco3Eh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"ba290c06-5ed6-4dab-f0c8-3f8bb1e7e504","executionInfo":{"status":"ok","timestamp":1589282815066,"user_tz":420,"elapsed":27214,"user":{"displayName":"Aravind Venugopal","photoUrl":"","userId":"09108547859103269258"}}},"source":["# model.save('/content/drive/My Drive/Colab Notebooks/Cloud 493/quora-similarity-model-dir')\n","import tensorflow as tf\n","model2 = tf.keras.models.load_model('/content/drive/My Drive/Colab Notebooks/Cloud 493/quora-similarity-model')\n","model2.save('/content/drive/My Drive/Colab Notebooks/Cloud 493/quora-similarity-model-dir', sign)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/Cloud 493/quora-similarity-model-dir/assets\n"],"name":"stdout"}]}]}